\documentclass[12pt,a4paper]{article}

% Package to include code
\usepackage{listings}
\usepackage{color}
\lstset{language=Python}
\lstset{numbers=none, basicstyle=\footnotesize,
  numberstyle=\tiny,keywordstyle=\color{blue},stringstyle=\ttfamily,showstringspaces=false}
\lstset{backgroundcolor=\color[rgb]{0.95 0.95 0.95}}
\lstdefinestyle{numbers}{numbers=left, stepnumber=1, numberstyle=\tiny, numbersep=10pt}
\lstdefinestyle{nonumbers}{numbers=none}


% Font selection: uncomment the next line to use the ``beton'' font
%\usepackage{beton}

% Font selection: uncomment the next line to use the ``times'' font
%\usepackage{times}

% Font for equations
\usepackage{euler}


%Package to define the headers and footers of the pages
\usepackage{fancyhdr}


%Package to include an index
\usepackage{index}

%Package to display boxes around texts. Used especially for the internal notes.
\usepackage{framed}

%PSTricks is a collection of PostScript-based TEX macros that is compatible
% with most TEX macro packages
\usepackage{pstricks}
\usepackage{pst-node}
\usepackage{pst-plot}
\usepackage{pst-tree}

%Package to display boxes around a minipage. Used especially to
%describe the biography of people.
\usepackage{boxedminipage}

%Package to include postscript figures
\usepackage{epsfig}

%Package for the bibliography
% \cite{XXX} produces Ben-Akiva et. al., 2010
% \citeasnoun{XXX} produces Ben-Akiva et al. (2010)
% \citeasnoun*{XXX} produces Ben-Akiva, Bierlaire, Bolduc and Walker (2010)
\usepackage[dcucite,abbr]{harvard}
\harvardparenthesis{none}\harvardyearparenthesis{round}

%Packages for advanced mathematics typesetting
\usepackage{amsmath,amsfonts,amssymb}

%Package to display trees easily
%\usepackage{xyling}

%Package to include smart references (on the next page, on the
%previous page, etc.) 
%%

%% Remove as it is not working when the book will be procesed by the
%% publisher.
%\usepackage{varioref}

%Package to display the euro sign
\usepackage[right,official]{eurosym}

%Rotate material, especially large table (defines sidewaystable)
\usepackage[figuresright]{rotating}

%Defines the subfigure environment, to obtain refs like Figure 1(a)
%and Figure 1(b). 
\usepackage{subfigure}

%Package for appendices. Allows subappendices, in particular
\usepackage{appendix}

%Package controling the fonts for the captions
\usepackage[font={small,sf}]{caption}

%Defines new types of columns for tabular ewnvironment
\usepackage{dcolumn}
\newcolumntype{d}{D{.}{.}{-1}}
\newcolumntype{P}[1]{>{#1\hspace{0pt}\arraybackslash}}
\newcolumntype{.}{D{.}{.}{9.3}}

%Allows multi-row cells in tables
\usepackage{multirow}

%Tables spaning more than one page
\usepackage{longtable}


%%
%%  Macros by Michel
%%

%Internal notes
\newcommand{\note}[1]{
\begin{framed}{}%
\textbf{\underline{Internal note}:} #1
\end{framed}}

%Use this version to turn off the notes
%\newcommand{\note}[1]{}


%Include a postscript figure . Note that the label is prefixed with
%``fig:''. Remember it when you refer to it.  
%Three arguments:
% #1 label
% #2 file (without extension)
% #3 Caption
\newcommand{\afigure}[3]{%
\begin{figure}[!tbp]%
\begin{center}%
\epsfig{figure=#2,width=0.8\textwidth}%
\end{center}
\caption{\label{fig:#1} #3}%
\end{figure}}






%Include two postscript figures side by side. 
% #1 label of the first figure
% #2 file for the first figure
% #3 Caption for the first figure
% #4 label of the second figure
% #5 file for the second figure
% #6 Caption for the first figure
% #7 Caption for the set of two figures
\newcommand{\twofigures}[7]{%
\begin{figure}[htb]%
\begin{center}%
\subfigure[\label{fig:#1}#3]{\epsfig{figure=#2,width=0.45\textwidth}}%
\hfill
\subfigure[\label{fig:#4}#6]{\epsfig{figure=#5,width=0.45\textwidth}}%
\end{center}
\caption{#7}%
\end{figure}}

%Include a figure generated by gnuplot using the epslatex output. Note that the label is prefixed with
%``fig:''. Remember it when you refer to it.  
 
%Three arguments:
% #1 label
% #2 file (without extension)
% #3 Caption
\newcommand{\agnuplotfigure}[3]{%
\begin{figure}[!tbp]%
\begin{center}%
\input{#2}%
\end{center}
\caption{\label{fig:#1} #3}%
\end{figure}}

%Three arguments:
% #1 label
% #2 file (without extension)
% #3 Caption
\newcommand{\asidewaysgnuplotfigure}[3]{%
\begin{sidewaysfigure}[!tbp]%
\begin{center}%
\input{#2}%
\end{center}
\caption{\label{fig:#1} #3}%
\end{sidewaysfigure}}


%Include two postscript figures side by side. 
% #1 label of the first figure
% #2 file for the first figure
% #3 Caption for the first figure
% #4 label of the second figure
% #5 file for the second figure
% #6 Caption for the second figure
% #7 Caption for the set of two figures
% #8 label for the whole figure
\newcommand{\twognuplotfigures}[7]{%
\begin{figure}[htb]%
\begin{center}%
\subfigure[\label{fig:#1}#3]{\input{#2}}%
\hfill
\subfigure[\label{fig:#4}#6]{\input{#5}}%
\end{center}
\caption{#7}%
\end{figure}}



%Include the description of somebody. Four arguments:
% #1 label
% #2 Name
% #3 file (without extension)
% #4 description
\newcommand{\people}[4]{
\begin{figure}[tbf]
\begin{boxedminipage}{\textwidth}
\parbox{0.40\textwidth}{\epsfig{figure=#3,width = 0.39\textwidth}}%\hfill
\parbox{0.59\textwidth}{%
#4% 
}%
\end{boxedminipage}
\caption{\label{fig:#1} #2}
\end{figure}
}

%Default command for a definition
% #1 label (prefix def:)
% #2 concept to be defined
% #3 definition
\newtheorem{definition}{Definition}
\newcommand{\mydef}[3]{%
\begin{definition}%
\index{#2|textbf}%
\label{def:#1}%
\textbf{#2} \slshape #3\end{definition}}

%Reference to a definitoin. Prefix 'def:' is assumed
\newcommand{\refdef}[1]{definition~\ref{def:#1}}


%Default command for a theorem, with proof
% #1: label (prefix thm:)
% #2: name of the theorem
% #3: statement
% #4: proof
\newtheorem{theorem}{Theorem}
\newcommand{\mytheorem}[4]{%
\begin{theorem}%
\index{#2|textbf}%
\index{Theorems!#2}%
\label{thm:#1}%
\textbf{#2} \sffamily \slshape #3
\end{theorem} \bpr #4 \epr \par}


%Default command for a theorem, without proof
% #1: label (prefix thm:)
% #2: name of the theorem
% #3: statement
\newcommand{\mytheoremsp}[3]{%
\begin{theorem}%
\index{#2|textbf}%
\index{Theorems!#2}%
\label{thm:#1}%
\textbf{#2} \sffamily \slshape #3
\end{theorem}}



%Put parentheses around the reference, as standard for equations
\newcommand{\req}[1]{(\ref{#1})}

%Short cut to make a column vector in math environment (centered)
\newcommand{\cvect}[1]{\left( \begin{array}{c} #1 \end{array} \right) }

%Short cut to make a column vector in math environment (right justified)
\newcommand{\rvect}[1]{\left( \begin{array}{r} #1 \end{array} \right) }

%A reference to a theorem. Prefix thm: is assumed for the label.
\newcommand{\refthm}[1]{theorem~\ref{thm:#1}}

%Reference to a figure. Prefix fig: is assumed for the label.
\newcommand{\reffig}[1]{Figure~\ref{fig:#1}}

%Smart reference to a figure. Prefix fig: is assumed for the label.
%\newcommand{\vreffig}[1]{Figure~\vref{fig:#1}}

%C in mathcal font for the choice set
\newcommand{\C}{\mathcal{C}}

%R in bold font for the set of real numbers
\newcommand{\R}{\mathbb{R}}

%N in bold font for the set of natural numbers
\newcommand{\N}{\mathbb{N}}

%C in mathcal font for the log likelihood
\renewcommand{\L}{\mathcal{L}}

%S in mathcal font for the subset S
\renewcommand{\S}{\mathcal{S}}

%To write an half in math envionment
\newcommand{\half}{\frac{1}{2}}

%Probability
\newcommand{\prob}{\operatorname{Pr}}

%Expectation
\newcommand{\expect}{\operatorname{E}}

%Variance
\newcommand{\var}{\operatorname{Var}}

%Covariance
\newcommand{\cov}{\operatorname{Cov}}

%Correlation
\newcommand{\corr}{\operatorname{Corr}}

%Span
\newcommand{\myspan}{\operatorname{span}}

%plim
\newcommand{\plim}{\operatorname{plim}}

%Displays n in bold (for the normal distribution?)
\newcommand{\n}{{\bf n}}

%Includes footnote in a table environment. Warning: the footmark is
%always 1.
\newcommand{\tablefootnote}[1]{\begin{flushright}
\rule{5cm}{1pt}\\
\footnotemark[1]{\footnotesize #1}
\end{flushright}
}

%Defines the ``th'' as in ``19th'' to be a superscript
\renewcommand{\th}{\textsuperscript{th}}

%Begin and end of a proof
\newcommand{\bpr}{{\bf Proof.} \hspace{1 em}}
\newcommand{\epr}{$\Box$}


\title{Monte-Carlo integration with PythonBiogeme}
\author{Michel Bierlaire} 
\date{August 6, 2015}

\newcommand{\PBIOGEME}{PythonBiogeme}


\begin{document}


\begin{titlepage}
\pagestyle{empty}

\maketitle
\vspace{2cm}

\begin{center}
\small Report TRANSP-OR 150806 \\ Transport and Mobility Laboratory \\ School of Architecture, Civil and Environmental Engineering \\ Ecole Polytechnique F\'ed\'erale de Lausanne \\ \verb+transp-or.epfl.ch+
\begin{center}
\textsc{Series on Biogeme}
\end{center}
\end{center}


\clearpage
\end{titlepage}

The package PythonBiogeme (\texttt{biogeme.epfl.ch}) is designed to estimate the parameters of
various models using maximum likelihood estimation. It is particularly
designed for discrete choice models. In this document, we investigate
some aspects related to Monte-Carlo integration, which is particularly
useful when estimating mixtures choice models, as well as choice
models with latent variables. We assume that the reader is already
familiar with discrete choice models, with \PBIOGEME, and with
simulation methods, although a short summary is provided.  This document has been written using
\PBIOGEME\ 2.4, but should remain valid for future versions.  

\section{Monte-Carlo integration}

Monte-Carlo integration consists in approximating an integral with the
sum of a
large number of terms. It comes from the definition of the expectation of
a continuous random variable. Consider the random variable $X$ with
probability density function (pdf) $f_X(x)$. Assuming that $X$ can take any
value in the interval $[a,b]$, where $a \in \R \cup \{ -\infty\}$ and $b
\in \R \cup \{+\infty \}$, the expected value of $X$ is given by
\begin{equation}
\label{eq:expect}
\expect[X] = \int_{a}^{b} x f_X(x) dx.
\end{equation}
Also, if $g:\R \to \R$ is a function, then 
\begin{equation}
\label{eq:expectg}
\expect[g(X)] = \int_{a}^{b} g(x) f_X(x) dx.
\end{equation}
The expectation of a random variable can be approximated using
simulation. The idea is simple: generate a sample of realizations of
$X$, that is generate $R$ draws $x_r$, $r=1,\ldots,
R$ from $X$, and calculate the sample mean:
\begin{equation}
\label{eq:meanmc}
\expect[g(X)] \approx \frac{1}{R} \sum_{r=1}^R g(x_r).
\end{equation}
Putting \req{eq:expectg} and \req{eq:meanmc} together, we obtain an
approximation to the integral:
\begin{equation}
 \int_{a}^{b} g(x) f_X(x) dx \approx  \frac{1}{R} \sum_{r=1}^R g(x_r).
\end{equation}
Also, we have 
\begin{equation}
 \int_{a}^{b} g(x) f_X(x) dx = \lim_{R\to\infty}  \frac{1}{R} \sum_{r=1}^R g(x_r).
\end{equation}
Therefore, the procedure to calculate the following integral
\begin{equation}
I = \int_a^b g(x) dx
\end{equation}
is the following
\begin{enumerate}
\item Select a random variable $X$ such that you can generate 
realizations of $X$, and such that the pdf $f_X$ is known;
\item Generate $R$ draws $x_r$, $r=1,\ldots,R$ from $X$;
\item Calculate
\begin{equation}
I \approx \frac{1}{R} \sum_{r=1}^R \frac{g(x_r)}{f_X(x_r)}.
\end{equation}
\end{enumerate}
In order to obtain an estimate of the approximation error, we must
calculate the variance the random variable.
The sample variance is an unbiased estimate of the true variance:
\begin{equation}
V_R = \frac{1}{R-1} \sum_{r=1}^R (\frac{g(x_r)}{f_X(x_r)} - I)^2.
\end{equation}
Alternatively as 
\begin{equation}
\var[g(X)] = \expect[g(X)^2] - \expect[g(x)]^2,
\end{equation}
the variance can be approximated by simulation as well:
\begin{equation}
\label{eq:simulatedVariance}
V_R \approx \frac{1}{R} \sum_{r=1}^R \frac{g(x_r)^2}{f_X(x_r)} -
I^2.
\end{equation}
Note that, for the values of $R$ that we are using in this document,
dividing by $R$ or by $R-1$ does not make much difference in practice. The approximation error is then estimated as
\begin{equation}
\label{eq:stderr}
e_R = \sqrt{\frac{V_r}{R}}.
\end{equation}
We refer the reader to \citeasnoun{Ross12} for a comprehensive
introduction to simulation methods. 

\section{Uniform draws}
\label{sec:uniform}
There are many algorithms  to draw from various
distributions. All of them require at some point draws from the
uniform distribution. There are several techniques that generate such
uniform draws.  In \PBIOGEME, one of them must be selected by setting the
parameter \lstinline$RandomDistribution$.

Each programming language provides a routine to
draw a random number between 0 and 1. Such routines are
deterministic, but the sequences of numbers that they generate share
many properties with sequences of random numbers. Therefore, they are
often called ``pseudo random numbers''.

\begin{lstlisting}
BIOGEME_OBJECT.PARAMETERS['RandomDistribution'] = "PSEUDO"
\end{lstlisting}


Researchers have proposed to use other types of sequences to perform
Monte-Carlo integration, called ``quasi-random sequences'' or
``low-discrepancy sequences''. \PBIOGEME\ implements the Halton draws,
from \citeasnoun{Halt60}.  They have been reported to perform well for
discrete choice models (\cite{Trai2000}, \cite{Bhat2001},
\cite{Bhat2003837}, \cite{Sandor2004313}). 

\begin{lstlisting}
BIOGEME_OBJECT.PARAMETERS['RandomDistribution'] = "HALTON"
\end{lstlisting}


The third method to generate uniform random numbers implemented in
\PBIOGEME\ is called ``Modified Latin Hypercube Sampling'', and has been
proposed by \citeasnoun{HessTraiPola05}.

\begin{lstlisting}
BIOGEME_OBJECT.PARAMETERS['RandomDistribution'] = "MHLS"
\end{lstlisting}


In the following, we are using these three options, and compare the
accuracy of the corresponding Monte-Carlo integration.


\section{Illustration with \PBIOGEME}
\label{sec:illustration}
We first illustrate the method on a simple integral.
Consider
\begin{equation}
I = \int_0^1 e^x dx.
\end{equation}
In this case, it can be solved analytically:
\begin{equation}
I = e - 1 = 1.7183.
\end{equation}
In order to use Monte-Carlo integration, we consider the random
variable $X$ that is uniformly distributed on $[0,1]$, so that
\begin{equation}
f_X(x) = \left\{
\begin{array}{ll}
1 & \text{if } x \in [0,1], \\
0 & \text{otherwise.}
\end{array}
\right.
\end{equation}
Therefore, we can approximate $I$ by generating $R$ draws from $X$ and
\begin{equation}
\label{eq:simpleMC}
I = \expect[e^X] \approx \frac{1}{R} \sum_{r=1}^R \frac{e^{x_r}}{f_X(x_r)} = \frac{1}{R} \sum_{r=1}^R e^{x_r}.
\end{equation}
Moreover, as
\begin{equation}
\begin{aligned}
\var[e^X] &= \expect[e^{2X}] - \expect[e^X]^2 \\
 &= \int_0^1 e^{2x}dx - (e-1)^2 \\
 &= (e^2-1)/2 - (e-1)^2 \\
 &= 0.2420356075,
\end{aligned}
\end{equation}
 the standard error  is  $0.0034787613$ for $R=20000$, and
 $0.0011000809$ for
$R=200000$. These theoretical values are estimated also below using \PBIOGEME. 


We use \PBIOGEME\ to calculate \req{eq:simpleMC}. Note that
\PBIOGEME\ requires a data file, which is not necessary in this simplistic
case. We
use the simulation mode of \PBIOGEME. It generates output for each
row of the data file. In our case, we just need one output, so that we
take any data file, and exclude all rows of the file except the first
one, using the following syntax:
\begin{lstlisting}
__rowId__ = Variable('__rowId__')
BIOGEME_OBJECT.EXCLUDE = __rowId__ >= 1
\end{lstlisting}
For this specific example, the data included in the file are
irrelevant. The generation of draws in \PBIOGEME\ is performed using
the command \lstinline$bioDraws('U')$, where the argument
\lstinline$'U'$ provides the name of the random variable associated
with the draws. The distribution of the random variable is specified
using the following syntax: 
\begin{lstlisting}
BIOGEME_OBJECT.DRAWS = { 'U': 'UNIFORM'}
\end{lstlisting}
Note that the valid keywords are 
\begin{itemize}
\item \lstinline$UNIFORM$, for a uniform
distribution on the interval $[0,1]$, 
\item \lstinline$UNIFORMSYM$, for a
uniform distribution on the interval $[-1,1]$, 
\item \lstinline$NORMAL$, for a standard normal distribution, and
\item \lstinline$TRUNCNORMAL$, for a truncated standard normal
  distribution. The truncation is defined by the parameter \lstinline$NormalTruncation$:
\begin{lstlisting}
BIOGEME_OBJECT.PARAMETERS['NormalTruncation'] = "1.96"
\end{lstlisting}
\end{itemize}
The integrand is defined by the following statement:
\begin{lstlisting}
integrand = exp(bioDraws('U'))
\end{lstlisting}
and the Monte-Carlo integration is obtained as follows:
\begin{lstlisting}
simulatedI = MonteCarlo(integrand)
\end{lstlisting}
The number of draws is defined by the parameter \lstinline$NbrOfDraws$:
\begin{lstlisting}
BIOGEME_OBJECT.PARAMETERS['NbrOfDraws'] = "20000"
\end{lstlisting}
We calculate as well the simulated variance, using
\req{eq:simulatedVariance}:
\begin{lstlisting}
sampleVariance = \
  MonteCarlo(integrand*integrand) - simulatedI * simulatedI
\end{lstlisting}
and the standard error \req{eq:stderr}:
\begin{lstlisting}
stderr = (sampleVariance / 20000.0)**0.5
\end{lstlisting}
Also, as we know the true value of the integral
\begin{lstlisting}
trueI = exp(1.0) - 1.0 
\end{lstlisting}
we can calculate the error:
\begin{lstlisting}
error = simulatedI - trueI
\end{lstlisting}
The calculation is obtained using the following statements:
\begin{lstlisting}
simulate = {'01 Simulated Integral': simulatedI,
            '02 Analytical Integral': trueI,
            '03 Sample variance': sampleVariance,
            '04 Std Error': stderr,
            '05 Error': error}

rowIterator('obsIter') 

BIOGEME_OBJECT.SIMULATE = Enumerate(simulate,'obsIter')
\end{lstlisting}
We obtain the following results:
\begin{center}
\begin{tabular}{rr@{.}l}
Simulated Integral & 1&72007 \\
Analytical Integral & 1&71828 \\
Sample variance & 0&240135 \\
Std Error & 0&00346508 \\
Error &0&00178739 \\
\end{tabular}
\end{center}
Remember that the true variance is $0.2420356075$, and the true standard
error is $0.0034787613$.
If we use ten times more draws, that is 200,000 draws, we obtain a
more precise value:
\begin{center}
\begin{tabular}{rr@{.}l}
Simulated Integral & 1&71902 \\
Analytical Integral & 1&71828 \\
Sample variance & 0&24175 \\
Std Error & 0&00109943 \\
Error &0&000739329
\end{tabular}
\end{center}
Remember that the true variance is $0.2420356075$, and the true standard
error is $0.0011000809$.
The complete specification file for \PBIOGEME\ is available in Appendix~\ref{sec:01simpleIntegral}.

\section{Variance reduction}

There are several techniques to reduce the variance of the draws used
for the Monte-Carlo integration. Reducing the variance improves the
precision of the approximation for the same number of
draws. Equivalently, they allow to use less draws to achieve the same
precision. We introduce two of them in this document: antithetic
draws, and control variates. As the focus of this document is on
\PBIOGEME, we urge the reader to read an introduction to variance
reduction methods in simulation, for instance in \citeasnoun{Ross12}.


\subsection{Antithetic draws}
\label{sec:antithetic}

Instead of drawing from $X$, consider two random variables $X_1$ and $X_2$, identically distributed
with pdf $f_X=f_{X_1}=f_{X_2}$, and define a new random variable
\begin{equation}
Y = \frac{X_1+X_2}{2}.
\end{equation}
 Then, as
$\expect[Y]=\expect[X_1]=\expect[X_2]=\expect[X]$, we can rewrite \req{eq:expect}
as follows:
\begin{equation}
\expect[Y] = \frac{1}{2} \expect[X_1] +
\frac{1}{2} \expect[X_2] = \expect[X] = \int_{a}^{b} x f_X(x) dx.
\end{equation}
The variance of this quantity is 
\begin{equation}
\label{eq:varY}
\var[Y] = \frac{1}{4}(\var(X_1)+\var(X_2)+2\cov(X_1,X_2)).
\end{equation}
If $X_1$ and $X_2$  are independent, this variance is equal to
\begin{equation}
\var[Y] = \frac{1}{2}\var[X].
\end{equation}
Therefore, using $Y$ for Monte-Carlo integration is associated with a
variance divided by two, but requires twice more draws ($R$ draws for $X_1$
and $R$ draws for $X_2$). It has no advantage on drawing directly $R$
draws from $X$. Formally, we can compare the standard errors of the
two methods for the same number of draws. Drawing $2R$ draws from $X$,
we obtain the following standard error:
\begin{equation}
\sqrt{\frac{\var[X]}{2R}}.
\end{equation}
Drawing $R$ draws from $X_1$ and $R$ draws from $X_2$ to generate $R$
draws from $Y$, we obtain the
same standard error
\begin{equation}
\sqrt{\frac{\var[Y]}{R}} = \sqrt{\frac{\var[X]}{2R}}. 
\end{equation}

 However, if the variables $X_1$ and
$X_2$ happen to be negatively correlated, that is if $\cov(X_1,X_2) <
0$, then $\var[Y] < \var[X] / 2$, and drawing from $Y$ reduces the
standard error. For instance, if $X_1$ is uniformly
distributed on $[0,1]$, then $X_2=1-X_1$ is also uniformly
distributed on $[0,1]$, and 
\begin{equation}
\cov(X_1,X_2)=\expect[X_1(1-X_1)]-\expect[X_1]\expect[1-X_1] =
-\frac{1}{12} < 0.
\end{equation}
If $X_1$ has a standard normal distribution, that is such that
$\expect[X_1]=0$ and $\var[X_1]=1$, then $X_2=-X_1$ has also a
standard normal distribution, and is 
negatively
correlated with $X_1$, as
\begin{equation}
\cov(X_1,X_2) = \expect[-X_1^2] - \expect[X_1]\expect[-X_1] = -1 < 0.
\end{equation}
The other advantage of this method is that we can recycle the
draws. Once we have generated the draws $x_r$ from $X_1$, the draws from $X_2$
are obtained using $1-x_r$ and $-x_r$, respectively. 

Now, we have to be careful when this technique is used for the general
case \req{eq:expectg}. Indeed, it must be verified first that $g(X_1)$
and $g(X_2)$ are indeed negatively correlated. And it is not
guaranteed by the fact that $X_1$ and $X_2$ are negatively
correlated. Consider two examples.

First, consider $g(X)=\left(x-\frac{1}{2} \right)^2$. Applying the
antithetic method with 
\begin{equation}
X_1 = \left(X-\frac{1}{2}\right)^2 \text{ and } X_2 = \left((1-X)-\frac{1}{2}\right)^2 
\end{equation}
does not work, as 
\begin{equation}
\cov(X_1,X_2) = \frac{1}{180} > 0.
\end{equation}
Actually, applying the antithetic method would \emph{increase} the
variance here, which is not desirable.

Second, consider $g(X)=e^X$, as in the example presented in
Section~\ref{sec:illustration}. We apply the antithetic method using 
\begin{equation}
Y = \frac{e^{X}+e^{1-X}}{2}.
\end{equation}
Here, the two transformed random variables are negatively correlated:
\begin{equation}
\begin{aligned}
\cov(e^X,e^{1-X}) &= \expect[e^X e^{1-X}] -
\expect[e^X]\expect[e^{1-X}] \\
 &= e - (e-1)^2 \\
 &= -0.2342106136.
\end{aligned}
\end{equation}
Therefore, the variance of $Y$ given by \req{eq:varY} is
$0.0039124969$, as opposed to $0.2420356075 / 2 = 0.1210178037$ if the
two sets of draws were independent. It means that for 10000 draws from
$Y$, the standard error decreases from $0.0034787613$ down to
$0.0006254996$. Moreover, as we use recycled draws, we need only 10000
draws instead of 20000. 

To apply this technique in \PBIOGEME, the integrand is defined as
follows:
\begin{lstlisting}
integrand = 0.5 * (exp(bioDraws('U')) + exp(1.0-bioDraws('U')))
\end{lstlisting}
and the number of draws reduced to 10000:
\begin{lstlisting}
stderr = (sampleVariance / 10000.0)**0.5
BIOGEME_OBJECT.PARAMETERS['NbrOfDraws'] = "10000"
\end{lstlisting}
We obtain the following results:
\begin{center}
\begin{tabular}{rr@{.}l}
Simulated Integral & 1&71708 \\
Analytical Integral &1&71828 \\
Sample variance & 0&00380337 \\
Std Error & 0&000616715 \\
Error & -0&00120542 \\
\end{tabular}
\end{center}
The reader can compare these values with the theoretical derivation
presented above. 
The complete specification file for \PBIOGEME\ is available in Appendix~\ref{sec:02antithetic}.

\subsection{Control variate}
\label{sec:cv}
The control variate method reduces the variance by exploiting
information from another random variable, correlated with $g(X)$, with a
known mean. Consider the random variable $Y$ such that
$\expect[Y]=\mu$.
We define a new random variable $Z$ as follows:
\begin{equation}
Z = g(X) + c(Y-\mu)
\end{equation}
where $c\in \R$ is a parameter. By construction, $\expect[Z]=\expect[g(X)]$
for any $c$, so that draws from $Z$ can be used instead of draws from
$g(X)$ for Monte-Carlo integration. Note that we do not need any
assumption on $g$ here.  The idea is to identify the value of
$c$ that minimizes the variance of $Z$. 
We have
\begin{equation}
\var[Z]=\var[g(X)+cY] = \var[g(X)] + c^2 \var[Y] + 2c\cov(g(X),Y),
\end{equation}
which is minimized for
\begin{equation}
c^* = -\frac{\cov(g(X),Y)}{\var[Y]}.
\end{equation}
Therefore, we use for Monte-Carlo integration the random variable
\begin{equation}
Z^* = g(X)  -\frac{\cov(g(X),Y)}{\var{Y}}(Y-\mu),
\end{equation}
with variance
\begin{equation}
\var[Z^*] = \var[g(X)] - \frac{\cov(g(X),Y)^2}{\var{Y}} \leq \var[g(X)].
\end{equation}
Note that, as for antithetic draws, this technique exploits the
correlation between two random variables. If $Y$ is independent from
$g(X)$, no variance reduction is achieved. 

In our example, $g(X)=e^X$. If we select $Y=X$, we know that 
\begin{equation}
\expect[Y] = \frac{1}{2} \text{ and } \var[Y] = \frac{1}{12}.
\end{equation}
Moreover, 
\begin{equation}
\cov(g(X),Y) = \cov(e^X,X) = (3-e)/2 = 0.1408590858.
\end{equation}
Therefore, we obtain
\begin{equation}
c^* = -\frac{\cov(g(X),Y)}{\var{Y}} = -6(3-e) = -1.6903090292,
\end{equation}
and the variance of $Z^*$ is $0.0039402229$, which is much lower than the
variance of $x$, that is $0.2420356075$. It means that, for 20000
draws, the standard error is $0.0004438594$, as opposed to
$0.0034787613$. With this method, only 326 draws are sufficient to
achieve the same precision as the Monte-Carlo integration without
control variate.  Indeed, 
\begin{equation}
\sqrt{\frac{0.0039402229}{326}} = 0.003476575.
\end{equation}
This is a tremendous saving. The control variate method is invoked in \PBIOGEME\ using the
following statement:
\begin{lstlisting}
simulatedI = MonteCarloControlVariate(integrand,bioDraws('U'),0.5),
\end{lstlisting}
where the second argument \lstinline$bioDraws('U')$ is $Y$, and the
third, \lstinline$0.5$, is $\mu$.
Note that, in addition to the output requested by the user,
\PBIOGEME\ also generates a report containing statistics on $g(X)$, $Y$ and
$Z^*$. In particular, it reports both the simulated value of $Y$ and
$\mu$ to detect any implementation error. 

The results of the Monte-Carlo integration are:
\begin{center}
\begin{tabular}{rr@{.}l}
Simulated Integral ($\expect[Z^*]$)& 1&71759 \\
Simulated Integral ($\expect[X]$)& 1&72007 \\
Analytical Integral & 1&71828 \\
Sample variance ($\var[X]$)& 0&239849 \\
Std Error ($\sqrt{\var[Z^*]/20000}$)& 0&000440564 \\
Error & -0&00069233
\end{tabular}
\end{center}
The complete specification file for \PBIOGEME\ is available in Appendix~\ref{sec:03controlVariate}.

Finally, we present in Table~\ref{tab:resultsSimple} the results of
the three methods, using different types of uniform draws as
described in Section~\ref{sec:uniform}. For each technique, the
standard errors for the three types of draws are comparable, with the
antithetic draws achieving the best value, followed by the control
variate. However, the precision actually achieved is much better for
Halton, and even more for MLHS.

\begin{table}[htb]
\begin{center}
\begin{tabular}{l|r@{.}lr@{.}lr@{.}l}
       & \multicolumn{2}{c}{Pseudo} &  \multicolumn{2}{c}{Halton} &
  \multicolumn{2}{c}{MHLS} \\
\hline 
Monte-Carlo    &  1&71902 &  1&71814 & 1&71829 \\
Standard error & 0&00109943 & 0&00109999 & 0&00110009 \\
Actual error   & 0&000739329 & -0&000145885 & 9&38555e-06 \\
\hline
Antithetic  & 1&71708 & 1&71828 & 1&71828 \\
Standard error & 0&000616715 & 0&000625455 & 0&0006255 \\
Actual error & -0&00120542 & -2&27865e-06 & -6&13416e-10 \\
\hline
Control variate & 1&71759 & 1&71828 & 1&71828 \\
Standard error & 0&000440564 & 0&000443827 & 0&000443872 \\
Actual error & -0&00069233 & -2&84647e-06 & 1&52591e-07 \\
\hline
\end{tabular}
\caption{\label{tab:resultsSimple}Comparison of variants of
  Monte-Carlo integration on the simple example}
\end{center}
\end{table}

We encourage the reader to perform similar  tests for other simple
integrals. For instance,
\begin{equation}
\int_0^1 \left(x-\frac{1}{2} \right)^2 dx = \frac{1}{12}
\end{equation}
or
\begin{equation}
\int_{-2}^2 \left( e^{-x} + \frac{1}{2+\varepsilon-x}\right)dx= e^2 -
e^{-2} + \log \frac{4+\varepsilon}{\varepsilon},
\end{equation}
where $\varepsilon > 0$. Note that the domain of integration is not $[0,1]$.

\section{Mixtures of logit}

Consider an
individual $n$, a choice set $\C_n$, and an alternative $i\in\C_n$. The
probability to choose $i$ is given by the choice model:
\begin{equation}
P_n(i|x,\theta,\C_n), 
\end{equation}
where $x$ is a vector of explanatory variables and $\theta$ is a vector
of parameters to be estimated from data. In the random utility framework,
a utility function is defined for each individual $n$ and each
alternative $i\in\C_n$:
\begin{equation}
U_{in}(x,\theta)= V_{in}(x,\theta) + \varepsilon_{in}(\theta),
\end{equation}
where $V_{in}(x,\theta)$ is deterministic and $\varepsilon_{in}$ is a
random variable independent from $x$. The model is then written:
\begin{equation}
P_n(i|x,\theta,\C_n) = \prob(U_{in}(x,\theta) \geq U_{jn}(x,\theta), \forall j \in \C_n).
\end{equation}
Specific models are obtained from assumptions about the distribution
of $\varepsilon_{in}$. Namely, if $\varepsilon_{in}$ are
i.i.d. (across both $i$ and $n$) extreme value distributed, we obtain
the logit model: 
\begin{equation}
P_n(i|x,\theta,\C_n) = \frac{e^{V_{in}(x,\theta)}}{\sum_{j\in\C_n}e^{V_{jn}(x,\theta)}}.
\end{equation}
Mixtures of logit are obtained when some of the parameters $\theta$
are distributed instead of being fixed. Denote
$\theta=(\theta_f,\theta_d)$, where $\theta_f$ is the vector of fixed
parameters, while $\theta_d$ is the vector of distributed parameters,
so that the choice model, conditional on $\theta_d$, is 
\begin{equation}
P_n(i|x,\theta_f,\theta_d,\C_n).
\end{equation}
A distribution is to be assumed for $\theta_d$. We denote the pdf of
this distribution by $f_{\theta_d}(\xi;\gamma)$, where $\gamma$ contains the
parameters of the distribution. Parameters $\gamma$ are sometimes
called the \emph{deep parameters} of the model. Therefore, the choice model becomes:
\begin{equation}
\label{eq:mixedLogit}
P_n(i|x,\theta_f,\gamma,\C_n) = \int_\xi P_n(i|x,\theta_f,\xi,\C_n) f_{\theta_d}(\xi)d\xi,
\end{equation}
where $\theta_f$ and $\gamma$ must be estimated from data.
The above integral has no analytical solution, even when the kernel
$P_n(i|x,\theta_f,\xi,\C_n)$ is a logit model. Therefore, it must be
calculated with numerical integration or Monte-Carlo integration. We
do both here to investigate the precision of the variants of
Monte-Carlo integration.  

\subsection{Comparison of integration methods on one integral}
We
consider the Swissmetro example (\cite{BierAxhaAbay01}). The data file
is available from \verb+biogeme.epfl.ch+. Consider the
following specification: 
\begin{itemize}
\item Variables $x$: see variables in the data file and new variables
  defined in Section~\ref{sec:05normalMixtureTrueAnalytical}. 
\item Fixed parameters $\theta_f$
\begin{lstlisting}
ASC_CAR = 0.137
ASC_TRAIN = -0.402
ASC_SM = 0
B_COST = -1.29
\end{lstlisting}
\item Deep parameters $\gamma$:
\begin{lstlisting}
B_TIME = -2.26
B_TIME_S = 1.66
\end{lstlisting}
\item We define the coefficient of travel time to be distributed,
  using the random variable \lstinline$omega$, that is assumed to be
  normally distributed:
\begin{lstlisting}
B_TIME_RND = B_TIME + B_TIME_S * omega
\end{lstlisting}
The parameter \lstinline$B_TIME$ is the mean of \lstinline$B_TIME_RND$, and
\lstinline$B_TIME_S$$^2$ is its variance. Note that
\lstinline$B_TIME_S$ is \textbf{not} the standard deviation, and can
be positive of negative. 
\item Utility functions $V_{in}$:
\begin{lstlisting}
V1 = ASC_TRAIN + \
     B_TIME_RND * TRAIN_TT_SCALED + \
     B_COST * TRAIN_COST_SCALED
V2 = ASC_SM + \
     B_TIME_RND * SM_TT_SCALED + \
     B_COST * SM_COST_SCALED
V3 = ASC_CAR + \
     B_TIME_RND * CAR_TT_SCALED + \
     B_COST * CAR_CO_SCALED
V = {1: V1, 2: V2, 3: V3}
\end{lstlisting}
\item Choice set $\C_n$, characterized by the availability conditions:
\begin{lstlisting}
CAR_AV_SP = \
  DefineVariable('CAR_AV_SP',CAR_AV  * (  SP   !=  0  ))
TRAIN_AV_SP = \
  DefineVariable('TRAIN_AV_SP',TRAIN_AV  * (  SP   !=  0  ))
av = {1: TRAIN_AV_SP,
      2: SM_AV,
      3: CAR_AV_SP}
\end{lstlisting}
\end{itemize}

As there is only one random parameter, the model \req{eq:mixedLogit}
can be calculated using numerical integration. It is done in
\PBIOGEME\ using the following procedure:
\begin{enumerate}
\item Mention that \lstinline$omega$ is a random variable:
\begin{lstlisting}
omega = RandomVariable('omega')
\end{lstlisting}
\item Define its pdf:
\begin{lstlisting}
density = normalpdf(omega).
\end{lstlisting}
Make sure that the library \lstinline$distributions$ is loaded in
order to use the function \lstinline$normalpdf$, using the following
statement:
\begin{lstlisting}
from distributions import *
\end{lstlisting}
\item Define the integrand from the logit model, where the probability
  of the alternative observed to be chosen is calculated (which is
  typical when calculating a likelihood function):
\begin{lstlisting}
integrand = bioLogit(V,av,CHOICE)
\end{lstlisting}
\item Calculate the integral:
\begin{lstlisting}
analyticalI = Integrate(integrand*density,'omega')
\end{lstlisting}
\end{enumerate}

The complete specification file for \PBIOGEME\ is available in
Appendix~\ref{sec:05normalMixtureTrueAnalytical}. The value of the
choice model for first observation in the data file is 
\begin{equation}
I =  \int_\xi P_n(i|x,\theta_f,\xi,\C_n) f_{\theta_d}(\xi)d\xi = 0.637849835578.
\end{equation}
Note that, in order ot obtain so many significant digits, we have used
the following statement:
\begin{lstlisting}
BIOGEME_OBJECT.PARAMETERS['decimalPrecisionForSimulation'] = "12"
\end{lstlisting}
To calculate the same integral with Monte-Carlo integration, we use
the same syntax as described earlier in this document:
\begin{lstlisting}
omega = bioDraws('B_TIME_RND')
BIOGEME_OBJECT.PARAMETERS['NbrOfDraws'] = "20000"
BIOGEME_OBJECT.DRAWS = { 'B_TIME_RND': 'NORMAL' }
B_TIME_RND = B_TIME + B_TIME_S * omega
integrand = bioLogit(V,av,CHOICE)
simulatedI = MonteCarlo(integrand)
\end{lstlisting}
The complete specification file for \PBIOGEME\ is available in
Appendix~\ref{sec:06normalMixture}. Using the result of the numerical integration as the ``true'' value of
the integral, We obtain the following results:
\begin{center}
\begin{tabular}{rr@{.}l}
Simulated integral & 0&637263 \\
Numerical integration & 0&63785 \\
Sample variance & 0&0299885 \\
Std Error & 0&000387224 \\
Error & -0&000586483 \\
\end{tabular}
\end{center}

We now apply the variance reduction methods. The antithetic draws
described in Section~\ref{sec:antithetic} are generated as follows:
\begin{enumerate}
\item As we are dealing with draws from the normal distribution, the
  antithetic draw of  $x_r$ is $-x_r$. We create two versions of the
  parameter, one with the draw, and one with its antithetic: 
\begin{lstlisting}
B_TIME_RND = B_TIME + B_TIME_S * bioDraws('B_TIME_RND')
B_TIME_RND_MINUS = B_TIME - B_TIME_S * bioDraws('B_TIME_RND')
\end{lstlisting}
\item Consistently, we then generate two versions of the model:
\begin{lstlisting}
V1_MINUS = ASC_TRAIN + \
           B_TIME_RND_MINUS * TRAIN_TT_SCALED + \
           B_COST * TRAIN_COST_SCALED
V2_MINUS = ASC_SM + \
           B_TIME_RND_MINUS * SM_TT_SCALED + \
           B_COST * SM_COST_SCALED
V3_MINUS = ASC_CAR + \
           B_TIME_RND_MINUS * CAR_TT_SCALED + \
           B_COST * CAR_CO_SCALED
V_MINUS = {1: V1_MINUS,
           2: V2_MINUS,
           3: V3_MINUS}
\end{lstlisting}
\item The integrand is the average of the integrands
  generated by the two versions of the model:
\begin{lstlisting}
integrand_plus = bioLogit(V,av,CHOICE)
integrand_minus = bioLogit(V_MINUS,av,CHOICE)
integrand = 0.5 * (integrand_plus + integrand_minus)
\end{lstlisting}
\end{enumerate}
The complete specification file for \PBIOGEME\ is available in
Appendix~\ref{sec:07normalMixtureAntithetic}.

The control variate method, described in Section~\ref{sec:cv},
requires an output of the simulation such that the analytical integral
is known. We propose here to consider
\begin{equation}
\int_0^1 e^{V_{in}(x,\theta_f,\xi)} d\xi =
\frac{e^{V_{in}(x,\theta_f,1)}-e^{V_{in}(x,\theta_f,0)}}{\partial V_{in}(x,\theta_f,\xi)/\partial
  \xi},
\end{equation}
if $\partial V_{in}(x,\theta_f,\xi)/\partial \xi$ does not depend on
$\xi$. This integral is calculated by Monte-Carlo after recycling the uniform
draws used to generate the normal draws for the original
integration. We follow the following procedure:
\begin{enumerate}
\item We recycle the draws:
\begin{lstlisting}
UNIFDRAW = bioRecycleDraws('B_TIME_RND')
\end{lstlisting}
\item We calculate the control variate integrand:
\begin{lstlisting}
VCV = ASC_TRAIN + \
      (B_TIME + B_TIME_S * UNIFDRAW) * TRAIN_TT_SCALED + \
      B_COST * TRAIN_COST_SCALED
\end{lstlisting}
Note that the derivative with respect to \lstinline$UNIFDRAW$ is
\begin{lstlisting}
B_TIME_S * TRAIN_TT_SCALED$.
\end{lstlisting}
\item We provide the analytical value of the control variate integral:
\begin{lstlisting}
VCV_ZERO = ASC_TRAIN + \
           B_TIME * TRAIN_TT_SCALED + \
           B_COST * TRAIN_COST_SCALED
VCV_ONE = ASC_TRAIN + \
         (B_TIME + B_TIME_S ) * TRAIN_TT_SCALED + \
         B_COST * TRAIN_COST_SCALED
VCV_INTEGRAL = (exp(VCV_ONE) - exp(VCV_ZERO)) / \
               (B_TIME_S * TRAIN_TT_SCALED)
\end{lstlisting}
\item We perform the Monte-Carlo integration:
\begin{lstlisting}
simulatedI = MonteCarloControlVariate(integrand,\
                                      exp(VCV),\
                                      VCV_INTEGRAL)
\end{lstlisting}
\end{enumerate}
The complete specification file for \PBIOGEME\ is available in
Appendix~\ref{sec:08normalMixtureControlVariate}.

Table~\ref{tab:resultsML} provides the results of the Monte-Carlo
integration using different variance reduction methods (none,
antithetic and control variates), different uniform draws (pseudo,
Halton and MLHS), and different number of draws. 

We can observe the following:
\begin{itemize}
\item In terms of standard errors of the draws, the Monte-Carlo
  integration without variance reduction has a standard error about
  7.5 times as large as the anthitetic version, and 2 times as large
  as the control variate.
\item In terms of absolute error, when compared to the value provided
  by the numerical integration, the error of the Monte-Carlo
  integration without variance  reduction is about the same for the pseudo
  draws, 12 times larger for the Halton draws, and 25 times larger for
  the MHLS draws, when compared to the antithetic draws. If it between
  3 and 4 times larger, when compared to the control variates. 
\item There is no significant
  difference in terms of standard errors  across the
  types of draws, irrespectively of the variance reduction method
  used. 
\item In terms of actual error, though, the Halton draws improves the
precision of the output, and the MHLS even more.  
\item Using the antithetic draws with 1000 draws achieves a similar
  precision as no variance reduction with 20000 draws.
\item Using the control variates with 2000 draws achieves a similar
  precision as no variance reduction with 20000 draws.
\item Reducing the number of draws to 500 does not deteriorate much
  the precision for the antithetic draws.
\end{itemize}

It would be useful to perform the same experiment for some other
observations in the data file. Such experiments can give useful
insights to for the choice of the most appropriate integration
technique. In the following, we compare some of these techniques for
the maximum likelihood estimation of the parameters of the model.

\begin{table}[htb]
\begin{center}
\begin{tabular}{lr|r@{.}lr@{.}lr@{.}l}
    & Draws   & \multicolumn{2}{c}{Pseudo} &  \multicolumn{2}{c}{Halton} &
  \multicolumn{2}{c}{MHLS} \\
\hline 
\multicolumn{8}{c}{20000 draws} \\
\hline
\textbf{Monte-Carlo}  & 20000   & 0&637263 & 0&637923 & 0&637845 \\
Standard error & & 0&000387224 & 0&000390176 & 0&000390301 \\
Actual error   & & -0&000586483 & 7&35443e-05 & -5&08236e-06 \\
\hline
\textbf{Antithetic} & 10000  & 0&638383 & 0&637856 & 0&63785 \\
Standard error & & 5&13243e-05 & 5&24484e-05 & 5&24949e-05 \\
Actual error & & 0&000533174 & 6&1286e-06 & 1&96217e-07 \\
\hline
\textbf{Control variate} & 20000 & 0&6377 & 0&637871 & 0&637848 \\
Standard error & & 0&000176759 & 0&000179054 & 0&00017928 \\
Actual error & & -0&000149889 & 	2&127e-05 & -1&72413e-06 \\
\hline
\multicolumn{8}{c}{2000 draws} \\
\hline
\textbf{Antithetic} & 1000  & 0&638783 & 0&637965  & 0&637853 \\
Standard error & & 5&05914e-05 & 5&17454e-05 & 5&24619e-05 \\
Actual error & & 0&000933592 & 0&000114998 & 3&32666e-06 \\
\hline
\textbf{Control variate} & 2000 & 0&637876 & 0&637975 & 0&637835 \\
Standard error & & 0&000551831  &0&00056032 & 0&000567009 \\
Actual error & &  2&66122e-05  &0&000125218 & -1&50796e-05 \\
\hline
\multicolumn{8}{c}{500 draws} \\
\hline
\textbf{Antithetic} & 250  & 0&639205 & 0&638459 &0&637869 \\
Standard error & & 5&17638e-05 & 4&97379e-05 & 5&23141e-05 \\
Actual error & & 0&00135483 & 0&000609069 & 1&87082e-05 \\
\hline
\textbf{Control variate} & 500 & 0&637587 & 0&638158 &0&637798 \\
Standard error & & 0&00111188 &	0&00109022 & 0&00113287 \\
Actual error & &  -0&000262395 & 0&000308626 & -5&2274e-05 \\
\hline
\end{tabular}
\caption{\label{tab:resultsML}Comparison of variants of
  Monte-Carlo integration on the mixture of logit example}
\end{center}
\end{table}

\subsection{Comparison of integration methods for maximum likelihood estimation}

We now estimate the parameters of the model using all observations
in the data set associated with work trips. Observations such that the dependent variable \lstinline$CHOICE$ is $0$ are also removed.
\begin{lstlisting}
exclude = (( PURPOSE != 1 ) * (  PURPOSE   !=  3  ) + \
          ( CHOICE == 0 )) > 0
BIOGEME_OBJECT.EXCLUDE = exclude
\end{lstlisting}
The estimation using numerical integration is performed using the following statements:
\begin{lstlisting}
integrand = bioLogit(V,av,CHOICE)
prob = Integrate(integrand*density,'omega')
l = log(prob)
rowIterator('obsIter') 
BIOGEME_OBJECT.ESTIMATE = Sum(l,'obsIter')
\end{lstlisting}
The complete specification file for \PBIOGEME\ is available in
Appendix~\ref{sec:11estimationNumerical}.

For Monte-Carlo integration, we use the following statements:
\begin{lstlisting}
prob = bioLogit(V,av,CHOICE)
l = mixedloglikelihood(prob)
rowIterator('obsIter') 
BIOGEME_OBJECT.ESTIMATE = Sum(l,'obsIter')
\end{lstlisting}
where the statement \lstinline$l = mixedloglikelihood(prob)$ is
 equivalent to
\begin{lstlisting}
integral = MonteCarlo(prob)
l = log(integral)
\end{lstlisting}
The complete specification file for \PBIOGEME\ is available in
Appendix~\ref{sec:12estimationMonteCarlo}.

The following estimation results are presented:
\begin{itemize}
\item Table \ref{tab:estNumerical}: numerical integration;
\item Table \ref{tab:estMC}:  Monte-Carlo integration,
  no variance reduction, 2000 MHLS draws;
\item Table \ref{tab:estAnti}: antithetic  draws, 1000 MHLS draws;
\item Table \ref{tab:estCV}:  control variates, 2000 MHLS draws;
\item Table \ref{tab:estMC500}:  Monte-Carlo integration, 500 MHLS draws;
\item Table \ref{tab:estAnti250}:  antithetic   draws, 250 MHLS draws;
\item Table \ref{tab:estCV500}: control variates, 500 MHLS draws.
\end{itemize}

The final log likelihood in each case, as well as the estimation time
are summarized in Table~\ref{tab:estimSummary}. In this experiment,
when looking at the estimates, it
seems that the MLHS draws provide relatively good precision, even for a lower
number of draws, and with no variance reduction. Clearly, this result
cannot be generalized, and should be investigated on a case by case
basis. Note however that the default type of draws in \PBIOGEME\ is
MLHS, because it is performing particularly well in this example.

\begin{table}[htb]
\begin{center}
\begin{tabular}{rrrrr}
Method & Draws &  Log likelihood & Run time \\
\hline
Numerical & --- & -5214.879 & 02:37 \\
Monte-Carlo & 2000 & -5214.835 & 31:11 \\
Antithetic & 1000 & -5214.899 & 39:26 \\
Control variate & 2000 & -5214.835 & 42:11 \\
Monte-Carlo & 500 & -5214.940 & 09:26    \\
Antithetic & 250 & -5214.897 & 09:21 \\
Control variate & 500 & -5214.940 & 08:59 \\
\end{tabular}
\end{center}
\caption{\label{tab:estimSummary}Final log likelihood and run time for
each integration method}
\end{table}

\begin{table}[htb]
  \begin{tabular}{l}
\begin{tabular}{rlr@{.}lr@{.}lr@{.}lr@{.}l}
         &                       &   \multicolumn{2}{l}{}    & \multicolumn{2}{l}{Robust}  &     \multicolumn{4}{l}{}   \\
Parameter &                       &   \multicolumn{2}{l}{Coeff.}      & \multicolumn{2}{l}{Asympt.}  &     \multicolumn{4}{l}{}   \\
number &  Description                     &   \multicolumn{2}{l}{estimate}      & \multicolumn{2}{l}{std. error}  &   \multicolumn{2}{l}{$t$-stat}  &   \multicolumn{2}{l}{$p$-value}   \\

\hline

1 & \lstinline$ASC_CAR$ & 0&137 & 0&0517 & 2&65 & 0&01\\
2 & \lstinline$ASC_TRAIN$ & -0&401 & 0&0656 & -6&12 & 0&00\\
3 & \lstinline$B_COST$ & -1&29 & 0&0863 & -14&90 & 0&00\\
4 & \lstinline$B_TIME$ & -2&26 & 0&117 & -19&38 & 0&00\\
5 & \lstinline$B_TIME_S$ & -1&65 & 0&125 & -13&26 & 0&00\\
\hline
\end{tabular}
\\
\begin{tabular}{rcl}
\multicolumn{3}{l}{\bf Summary statistics}\\
\multicolumn{3}{l}{ Number of observations = $6768$} \\
\multicolumn{3}{l}{ Number of excluded observations = $3960$} \\
\multicolumn{3}{l}{ Number of estimated  parameters = $5$} \\
\multicolumn{3}{l}{ Number of iterations = $13$} \\
\multicolumn{3}{l}{ Estimation time: $00:02:37$} \\
 $\mathcal{L}(\beta_0)$ &=&  $-7157.671$ \\
 $\mathcal{L}(\hat{\beta})$ &=& $-5214.879 $  \\
 $-2[\mathcal{L}(\beta_0) -\mathcal{L}(\hat{\beta})]$ &=& $3885.585$ \\
    $\rho^2$ &=&   $0.271$ \\
    $\bar{\rho}^2$ &=&    $0.271$ \\
\end{tabular}
  \end{tabular}

\caption{\label{tab:estNumerical}Estimation results with numerical integration}
\end{table}

\begin{table}[htb]
  \begin{tabular}{l}
\begin{tabular}{rlr@{.}lr@{.}lr@{.}lr@{.}l}
         &                       &   \multicolumn{2}{l}{}    & \multicolumn{2}{l}{Robust}  &     \multicolumn{4}{l}{}   \\
Parameter &                       &   \multicolumn{2}{l}{Coeff.}      & \multicolumn{2}{l}{Asympt.}  &     \multicolumn{4}{l}{}   \\
number &  Description                     &   \multicolumn{2}{l}{estimate}      & \multicolumn{2}{l}{std. error}  &   \multicolumn{2}{l}{$t$-stat}  &   \multicolumn{2}{l}{$p$-value}   \\

\hline

1 & \lstinline$ASC_CAR$ & 0&137 & 0&0517 & 2&65 & 0&01\\
2 & \lstinline$ASC_TRAIN$ & -0&402 & 0&0658 & -6&10 & 0&00\\
3 & \lstinline$B_COST$ & -1&29 & 0&0864 & -14&89 & 0&00\\
4 & \lstinline$B_TIME$ & -2&26 & 0&117 & -19&31 & 0&00\\
5 & \lstinline$B_TIME_S$ & 1&66 & 0&132 & 12&59 & 0&00\\
\hline
\end{tabular}
\\
\begin{tabular}{rcl}
\multicolumn{3}{l}{\bf Summary statistics}\\
\multicolumn{3}{l}{ Number of observations = $6768$} \\
\multicolumn{3}{l}{ Number of excluded observations = $3960$} \\
\multicolumn{3}{l}{ Number of estimated  parameters = $5$} \\
\multicolumn{3}{l}{ Number of iterations = $9$} \\
\multicolumn{3}{l}{ Estimation time: $00:31:11$} \\
 $\mathcal{L}(\beta_0)$ &=&  $-6964.663$ \\
 $\mathcal{L}(\hat{\beta})$ &=& $-5214.835 $  \\
 $-2[\mathcal{L}(\beta_0) -\mathcal{L}(\hat{\beta})]$ &=& $3499.656$ \\
    $\rho^2$ &=&   $0.251$ \\
    $\bar{\rho}^2$ &=&    $0.251$ \\
\end{tabular}
  \end{tabular}
\caption{\label{tab:estMC}Estimation results with Monte-Carlo,
  no variance reduction, 2000 MHLS draws}
\end{table}

\begin{table}[htb]
  \begin{tabular}{l}
\begin{tabular}{rlr@{.}lr@{.}lr@{.}lr@{.}l}
         &                       &   \multicolumn{2}{l}{}    & \multicolumn{2}{l}{Robust}  &     \multicolumn{4}{l}{}   \\
Parameter &                       &   \multicolumn{2}{l}{Coeff.}      & \multicolumn{2}{l}{Asympt.}  &     \multicolumn{4}{l}{}   \\
number &  Description                     &   \multicolumn{2}{l}{estimate}      & \multicolumn{2}{l}{std. error}  &   \multicolumn{2}{l}{$t$-stat}  &   \multicolumn{2}{l}{$p$-value}   \\

\hline

1 & \lstinline$ASC_CAR$ & 0&137 & 0&0517 & 2&65 & 0&01\\
2 & \lstinline$ASC_TRAIN$ & -0&402 & 0&0658 & -6&10 & 0&00\\
3 & \lstinline$B_COST$ & -1&29 & 0&0863 & -14&89 & 0&00\\
4 & \lstinline$B_TIME$ & -2&26 & 0&117 & -19&31 & 0&00\\
5 & \lstinline$B_TIME_S$ & 1&66 & 0&132 & 12&59 & 0&00\\
\hline
\end{tabular}
\\
\begin{tabular}{rcl}
\multicolumn{3}{l}{\bf Summary statistics}\\
\multicolumn{3}{l}{ Number of observations = $6768$} \\
\multicolumn{3}{l}{ Number of excluded observations = $3960$} \\
\multicolumn{3}{l}{ Number of estimated  parameters = $5$} \\
\multicolumn{3}{l}{ Number of iterations = $12$} \\
\multicolumn{3}{l}{ Estimation time: $00:39:26$} \\
 $\mathcal{L}(\beta_0)$ &=&  $-7155.875$ \\
 $\mathcal{L}(\hat{\beta})$ &=& $-5214.899 $  \\
 $-2[\mathcal{L}(\beta_0) -\mathcal{L}(\hat{\beta})]$ &=& $3881.952$ \\
    $\rho^2$ &=&   $0.271$ \\
    $\bar{\rho}^2$ &=&    $0.271$ \\
\end{tabular}
  \end{tabular}
\caption{\label{tab:estAnti}Estimation results with antithetic
  draws, 1000 MHLS draws}
\end{table}


\begin{table}[htb]
  \begin{tabular}{l}
\begin{tabular}{rlr@{.}lr@{.}lr@{.}lr@{.}l}
         &                       &   \multicolumn{2}{l}{}    & \multicolumn{2}{l}{Robust}  &     \multicolumn{4}{l}{}   \\
Parameter &                       &   \multicolumn{2}{l}{Coeff.}      & \multicolumn{2}{l}{Asympt.}  &     \multicolumn{4}{l}{}   \\
number &  Description                     &   \multicolumn{2}{l}{estimate}      & \multicolumn{2}{l}{std. error}  &   \multicolumn{2}{l}{$t$-stat}  &   \multicolumn{2}{l}{$p$-value}   \\

\hline

1 & \lstinline$ASC_CAR$ & 0&137 & 0&0517 & 2&65 & 0&01\\
2 & \lstinline$ASC_TRAIN$ & -0&402 & 0&0658 & -6&10 & 0&00\\
3 & \lstinline$B_COST$ & -1&29 & 0&0864 & -14&89 & 0&00\\
4 & \lstinline$B_TIME$ & -2&26 & 0&117 & -19&31 & 0&00\\
5 & \lstinline$B_TIME_S$ & 1&66 & 0&132 & 12&59 & 0&00\\
\hline
\end{tabular}
\\
\begin{tabular}{rcl}
\multicolumn{3}{l}{\bf Summary statistics}\\
\multicolumn{3}{l}{ Number of observations = $6768$} \\
\multicolumn{3}{l}{ Number of excluded observations = $3960$} \\
\multicolumn{3}{l}{ Number of estimated  parameters = $5$} \\
\multicolumn{3}{l}{ Number of iterations = $12$} \\
\multicolumn{3}{l}{ Estimation time: $00:42:11$} \\
 $\mathcal{L}(\beta_0)$ &=&  $-7155.867$ \\
 $\mathcal{L}(\hat{\beta})$ &=& $-5214.835 $  \\
 $-2[\mathcal{L}(\beta_0) -\mathcal{L}(\hat{\beta})]$ &=& $3882.063$ \\
    $\rho^2$ &=&   $0.271$ \\
    $\bar{\rho}^2$ &=&    $0.271$ \\
\end{tabular}
  \end{tabular}
\caption{\label{tab:estCV}Estimation results with control variates, 2000 MHLS draws}
\end{table}


\begin{table}[htb]
  \begin{tabular}{l}
\begin{tabular}{rlr@{.}lr@{.}lr@{.}lr@{.}l}
         &                       &   \multicolumn{2}{l}{}    & \multicolumn{2}{l}{Robust}  &     \multicolumn{4}{l}{}   \\
Parameter &                       &   \multicolumn{2}{l}{Coeff.}      & \multicolumn{2}{l}{Asympt.}  &     \multicolumn{4}{l}{}   \\
number &  Description                     &   \multicolumn{2}{l}{estimate}      & \multicolumn{2}{l}{std. error}  &   \multicolumn{2}{l}{$t$-stat}  &   \multicolumn{2}{l}{$p$-value}   \\

\hline

1 & \lstinline$ASC_CAR$ & 0&137 & 0&0517 & 2&65 & 0&01\\
2 & \lstinline$ASC_TRAIN$ & -0&402 & 0&0658 & -6&10 & 0&00\\
3 & \lstinline$B_COST$ & -1&29 & 0&0864 & -14&88 & 0&00\\
4 & \lstinline$B_TIME$ & -2&26 & 0&117 & -19&33 & 0&00\\
5 & \lstinline$B_TIME_S$ & 1&66 & 0&131 & 12&63 & 0&00\\
\hline
\end{tabular}
\\
\begin{tabular}{rcl}
\multicolumn{3}{l}{\bf Summary statistics}\\
\multicolumn{3}{l}{ Number of observations = $6768$} \\
\multicolumn{3}{l}{ Number of excluded observations = $3960$} \\
\multicolumn{3}{l}{ Number of estimated  parameters = $5$} \\
\multicolumn{3}{l}{ Number of iterations = $12$} \\
\multicolumn{3}{l}{ Estimation time: $00:09:26$} \\
 $\mathcal{L}(\beta_0)$ &=&  $-7155.962$ \\
 $\mathcal{L}(\hat{\beta})$ &=& $-5214.940 $  \\
 $-2[\mathcal{L}(\beta_0) -\mathcal{L}(\hat{\beta})]$ &=& $3882.044$ \\
    $\rho^2$ &=&   $0.271$ \\
    $\bar{\rho}^2$ &=&    $0.271$ \\
\end{tabular}
  \end{tabular}
\caption{\label{tab:estMC500}Estimation results with Monte-Carlo
  integration, no variance reduction, 500 MHLS draws}
\end{table}

\begin{table}[htb]
  \begin{tabular}{l}
\begin{tabular}{rlr@{.}lr@{.}lr@{.}lr@{.}l}
         &                       &   \multicolumn{2}{l}{}    & \multicolumn{2}{l}{Robust}  &     \multicolumn{4}{l}{}   \\
Parameter &                       &   \multicolumn{2}{l}{Coeff.}      & \multicolumn{2}{l}{Asympt.}  &     \multicolumn{4}{l}{}   \\
number &  Description                     &   \multicolumn{2}{l}{estimate}      & \multicolumn{2}{l}{std. error}  &   \multicolumn{2}{l}{$t$-stat}  &   \multicolumn{2}{l}{$p$-value}   \\

\hline

1 & \lstinline$ASC_CAR$ & 0&137 & 0&0517 & 2&65 & 0&01\\
2 & \lstinline$ASC_TRAIN$ & -0&402 & 0&0658 & -6&11 & 0&00\\
3 & \lstinline$B_COST$ & -1&29 & 0&0864 & -14&88 & 0&00\\
4 & \lstinline$B_TIME$ & -2&26 & 0&117 & -19&33 & 0&00\\
5 & \lstinline$B_TIME_S$ & 1&66 & 0&131 & 12&61 & 0&00\\
\hline
\end{tabular}
\\
\begin{tabular}{rcl}
\multicolumn{3}{l}{\bf Summary statistics}\\
\multicolumn{3}{l}{ Number of observations = $6768$} \\
\multicolumn{3}{l}{ Number of excluded observations = $3960$} \\
\multicolumn{3}{l}{ Number of estimated  parameters = $5$} \\
\multicolumn{3}{l}{ Number of iterations = $12$} \\
\multicolumn{3}{l}{ Estimation time: $00:09:21$} \\
 $\mathcal{L}(\beta_0)$ &=&  $-7155.877$ \\
 $\mathcal{L}(\hat{\beta})$ &=& $-5214.897 $  \\
 $-2[\mathcal{L}(\beta_0) -\mathcal{L}(\hat{\beta})]$ &=& $3881.960$ \\
    $\rho^2$ &=&   $0.271$ \\
    $\bar{\rho}^2$ &=&    $0.271$ \\
\end{tabular}
  \end{tabular}
\caption{\label{tab:estAnti250}Estimation results with antithetic
  draws, 250 MHLS draws}
\end{table}

\begin{table}[htb]
  \begin{tabular}{l}
\begin{tabular}{rlr@{.}lr@{.}lr@{.}lr@{.}l}
         &                       &   \multicolumn{2}{l}{}    & \multicolumn{2}{l}{Robust}  &     \multicolumn{4}{l}{}   \\
Parameter &                       &   \multicolumn{2}{l}{Coeff.}      & \multicolumn{2}{l}{Asympt.}  &     \multicolumn{4}{l}{}   \\
number &  Description                     &   \multicolumn{2}{l}{estimate}      & \multicolumn{2}{l}{std. error}  &   \multicolumn{2}{l}{$t$-stat}  &   \multicolumn{2}{l}{$p$-value}   \\

\hline

1 & \lstinline$ASC_CAR$ & 0&137 & 0&0517 & 2&65 & 0&01\\
2 & \lstinline$ASC_TRAIN$ & -0&402 & 0&0658 & -6&10 & 0&00\\
3 & \lstinline$B_COST$ & -1&29 & 0&0864 & -14&88 & 0&00\\
4 & \lstinline$B_TIME$ & -2&26 & 0&117 & -19&33 & 0&00\\
5 & \lstinline$B_TIME_S$ & 1&66 & 0&131 & 12&63 & 0&00\\
\hline
\end{tabular}
\\
\begin{tabular}{rcl}
\multicolumn{3}{l}{\bf Summary statistics}\\
\multicolumn{3}{l}{ Number of observations = $6768$} \\
\multicolumn{3}{l}{ Number of excluded observations = $3960$} \\
\multicolumn{3}{l}{ Number of estimated  parameters = $5$} \\
\multicolumn{3}{l}{ Number of iterations = $12$} \\
\multicolumn{3}{l}{ Estimation time: $00:08:59$} \\
 $\mathcal{L}(\beta_0)$ &=&  $-7155.962$ \\
 $\mathcal{L}(\hat{\beta})$ &=& $-5214.940 $  \\
 $-2[\mathcal{L}(\beta_0) -\mathcal{L}(\hat{\beta})]$ &=& $3882.044$ \\
    $\rho^2$ &=&   $0.271$ \\
    $\bar{\rho}^2$ &=&    $0.271$ \\
\end{tabular}
  \end{tabular}
\caption{\label{tab:estCV500}Estimation results with control variates, 500 MHLS draws}
\end{table}

\section{Conclusion}

This document describes the variants of Monte-Carlo integration, and
suggests how to perform some analysis using the \lstinline$SIMULATE$
operator of \PBIOGEME, that helps investigating the performance of
each of them before starting a maximum likelihod  estimation, that may
take a while to converge. In the example provided in this document,
the antithetic draws method, combined with MLHS appeared to be the
most precise. This result is not universal. The analysis must be
performed on a case by case basis.

\clearpage

\appendix

\section{Complete specification files}

\subsection{\lstinline$01simpleIntegral.py$}
\label{sec:01simpleIntegral}
\lstinputlisting[style=numbers]{../../../test/monteCarlo/01simpleIntegral.py}

\subsection{\lstinline$02antithetic.py$}
\label{sec:02antithetic}
\lstinputlisting[style=numbers]{../../../test/monteCarlo/02antithetic.py}

\subsection{\lstinline$03controlVariate.py$}
\label{sec:03controlVariate}
\lstinputlisting[style=numbers]{../../../test/monteCarlo/03controlVariate.py}

\subsection{\lstinline$05normalMixtureTrueAnalytical.py$}
\label{sec:05normalMixtureTrueAnalytical}
\lstinputlisting[style=numbers]{../../../test/monteCarlo/05normalMixtureTrueAnalytical.py}

\subsection{\lstinline$06normalMixture.py$}
\label{sec:06normalMixture}
\lstinputlisting[style=numbers]{../../../test/monteCarlo/06normalMixture.py}

\subsection{\lstinline$07normalMixtureAntithetic.py$}
\label{sec:07normalMixtureAntithetic}
\lstinputlisting[style=numbers]{../../../test/monteCarlo/07normalMixtureAntithetic.py}

\subsection{\lstinline$08normalMixtureControlVariate.py$}
\label{sec:08normalMixtureControlVariate}
\lstinputlisting[style=numbers]{../../../test/monteCarlo/08normalMixtureControlVariate.py}


\subsection{\lstinline$11estimationNumerical.py$}
\label{sec:11estimationNumerical}
\lstinputlisting[style=numbers]{../../../test/monteCarlo/11estimationNumerical.py}

\subsection{\lstinline$12estimationMonteCarlo.py$}
\label{sec:12estimationMonteCarlo}
\lstinputlisting[style=numbers]{../../../test/monteCarlo/12estimationMonteCarlo.py}




\clearpage 

\bibliographystyle{dcu}
\bibliography{../dca}





\end{document}





